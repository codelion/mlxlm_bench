#!/usr/bin/env python3
"""
MLX-LM Benchmark Tool

This script benchmarks the performance of an MLX-LM model by generating synthetic
prompt tokens and capturing generation metrics (e.g., tokens-per-second, peak memory usage).
It supports multiple output formats (CSV, JSON, JSONL, Markdown) and performs repeated runs
to average the results.
"""

import argparse
import json
import logging
import random
import re
import io
import contextlib
import csv
from typing import Any, Dict, List, Optional, Tuple

import mlx.core as mx
import mlx.nn as nn
from mlx_lm import load, generate

# Configure logging for better observability
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")


def load_model_tokenizer(model_path: str) -> Tuple[Any, Any]:
    """
    Load the MLX-LM model and its associated tokenizer.

    Args:
        model_path (str): Path or identifier of the model.

    Returns:
        Tuple[Any, Any]: The loaded model and tokenizer.
    """
    logging.info(f"Loading model from: {model_path}...")
    model, tokenizer = load(model_path)
    return model, tokenizer


def generate_synthetic_tokens(tokenizer: Any, seq_length: int) -> List[int]:
    """
    Generate a synthetic sequence of tokens using the tokenizer's vocabulary.

    Args:
        tokenizer (Any): The tokenizer instance with vocabulary details.
        seq_length (int): Desired total number of tokens.

    Returns:
        List[int]: List of token IDs forming the synthetic sequence.
    """
    vocab_size = tokenizer.vocab_size

    # Prepend BOS token if available; otherwise, start with an empty list.
    tokens = [tokenizer.bos_token_id] if tokenizer.bos_token_id is not None else []
    tokens += [random.randint(0, vocab_size - 1) for _ in range(seq_length - len(tokens))]

    return tokens


def parse_metrics(log_output: str) -> Dict[str, Optional[float]]:
    """
    Parse performance metrics from the log output generated by the `generate()` function.

    Args:
        log_output (str): Captured stdout containing performance logs.

    Returns:
        Dict[str, Optional[float]]: Parsed metrics including:
            - prompt_tokens: Number of prompt tokens processed.
            - prompt_tps: Prompt tokens processed per second.
            - response_tokens: Number of tokens generated.
            - response_tps: Generation tokens processed per second.
            - ram_usage: Peak memory usage in GB.
            - exec_time: Estimated execution time (s).
    """
    metrics: Dict[str, Optional[float]] = {
        "prompt_tokens": None,
        "prompt_tps": None,
        "response_tokens": None,
        "response_tps": None,
        "ram_usage": None,
        "exec_time": None
    }

    # Extract prompt tokens and tokens-per-second
    prompt_match = re.search(r"Prompt:\s*(\d+)\s*tokens,\s*([\d.]+)\s*tokens-per-sec", log_output)
    if prompt_match:
        metrics["prompt_tokens"] = int(prompt_match.group(1))
        metrics["prompt_tps"] = float(prompt_match.group(2))

    # Extract generation tokens and tokens-per-second
    generation_match = re.search(r"Generation:\s*(\d+)\s*tokens,\s*([\d.]+)\s*tokens-per-sec", log_output)
    if generation_match:
        metrics["response_tokens"] = int(generation_match.group(1))
        metrics["response_tps"] = float(generation_match.group(2))

    # Extract peak memory usage (GB)
    mem_match = re.search(r"Peak memory:\s*([\d.]+)\s*GB", log_output)
    if mem_match:
        metrics["ram_usage"] = float(mem_match.group(1))

    # Calculate total execution time if all relevant metrics are available
    if (metrics["prompt_tokens"] is not None and metrics["prompt_tps"] is not None and
        metrics["response_tokens"] is not None and metrics["response_tps"] is not None):
        metrics["exec_time"] = (metrics["prompt_tokens"] / metrics["prompt_tps"]) + (
            metrics["response_tokens"] / metrics["response_tps"])

    return metrics


def benchmark_performance(model: Any, tokenizer: Any, seq_length: int, max_tokens: int) -> Dict[str, Optional[float]]:
    """
    Run a single benchmark iteration, capturing performance metrics.

    Args:
        model (Any): The loaded MLX-LM model.
        tokenizer (Any): The associated tokenizer.
        seq_length (int): Number of synthetic prompt tokens.
        max_tokens (int): Maximum number of tokens to generate.

    Returns:
        Dict[str, Optional[float]]: Performance metrics from this iteration.
    """
    input_tokens = generate_synthetic_tokens(tokenizer, seq_length)

    # Capture stdout from generate() to parse the performance logs
    output_buffer = io.StringIO()
    with contextlib.redirect_stdout(output_buffer):
        generate(model, tokenizer, input_tokens, max_tokens=max_tokens, verbose=True)

    captured_output = output_buffer.getvalue()
    return parse_metrics(captured_output)


def save_results(results: Dict[str, Any], output_format: str) -> None:
    """
    Save the benchmark results in the specified output format.

    Args:
        results (Dict[str, Any]): Aggregated benchmark results.
        output_format (str): Format to save the results ("csv", "json", "jsonl", or "md").
    """
    if output_format == "json":
        with open("benchmark_results.json", "w") as f:
            json.dump(results, f, indent=4)
        logging.info("Results saved to benchmark_results.json")
    elif output_format == "jsonl":
        with open("benchmark_results.jsonl", "a") as f:
            f.write(json.dumps(results) + "\n")
        logging.info("Results appended to benchmark_results.jsonl")
    elif output_format == "csv":
        with open("benchmark_results.csv", "w", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["Model", "Prompt Tokens", "Prompt TPS", "Response Tokens",
                             "Response TPS", "Execution Time (s)", "Memory Usage (GB)"])
            writer.writerow([
                results.get("Model"),
                results.get("Prompt Tokens"),
                results.get("Prompt TPS"),
                results.get("Response Tokens"),
                results.get("Response TPS"),
                results.get("Execution Time (s)"),
                results.get("Memory Usage (GB)")
            ])
        logging.info("Results saved to benchmark_results.csv")
    elif output_format == "md":
        with open("benchmark_results.md", "w") as f:
            f.write("| Metric | Value |\n")
            f.write("| --- | --- |\n")
            for key, value in results.items():
                f.write(f"| {key} | {value} |\n")
        logging.info("Results saved to benchmark_results.md")
    else:
        logging.warning(f"Unsupported output format: {output_format}")


def run_benchmark(args: argparse.Namespace) -> Dict[str, Any]:
    """
    Execute the benchmark process including warmup and repeated iterations.

    Args:
        args (argparse.Namespace): Command-line arguments.

    Returns:
        Dict[str, Any]: Aggregated benchmark results.
    """
    model, tokenizer = load_model_tokenizer(args.model)

    # --- Warmup Run ---
    logging.info("Running warmup run...")
    _ = benchmark_performance(model, tokenizer, args.n_prompt, args.n_gen)
    logging.info("Warmup complete.")

    # --- Benchmark Iterations ---
    logging.info("Starting benchmark iterations...")
    metrics_list: List[Dict[str, Optional[float]]] = []
    for i in range(args.repetitions):
        logging.info(f"Iteration {i + 1}/{args.repetitions}...")
        metrics = benchmark_performance(model, tokenizer, args.n_prompt, args.n_gen)
        metrics_list.append(metrics)

    # Compute average metrics from all iterations
    avg_metrics: Dict[str, Optional[float]] = {}
    metric_keys = ["prompt_tokens", "prompt_tps", "response_tokens", "response_tps", "exec_time", "ram_usage"]
    for key in metric_keys:
        valid_values = [m[key] for m in metrics_list if m[key] is not None]
        avg_metrics[key] = sum(valid_values) / len(valid_values) if valid_values else None

    results = {
        "Model": args.model,
        "Prompt Tokens": int(avg_metrics["prompt_tokens"]),
        "Prompt TPS": round(avg_metrics["prompt_tps"], 3),
        "Response Tokens": int(avg_metrics["response_tokens"]),
        "Response TPS": round(avg_metrics["response_tps"], 3),
        "Execution Time (s)": round(avg_metrics["exec_time"], 3),
        "Memory Usage (GB)": round(avg_metrics["ram_usage"], 2) if avg_metrics["ram_usage"] is not None else None
    }

    logging.info("Benchmark completed. Averaged results:")
    logging.info(json.dumps(results, indent=4))

    # Save results in the chosen format
    save_results(results, args.output)

    return results


def parse_args() -> argparse.Namespace:
    """
    Parse command-line arguments for configuring the benchmark.

    Returns:
        argparse.Namespace: The parsed command-line arguments.
    """
    parser = argparse.ArgumentParser(description="Benchmark MLX-LM model performance.")
    parser.add_argument(
        "-m", "--model", type=str, default="mlx-community/llama2-7b-mlx",
        help="Path to the MLX model to benchmark."
    )
    parser.add_argument(
        "-p", "--n-prompt", type=int, default=512,
        help="Number of synthetic prompt tokens."
    )
    parser.add_argument(
        "-n", "--n-gen", type=int, default=128,
        help="Number of tokens to generate."
    )
    parser.add_argument(
        "-o", "--output", type=str, choices=["csv", "json", "jsonl", "md"],
        default="md", help="Output format for the benchmark results."
    )
    parser.add_argument(
        "-r", "--repetitions", type=int, default=5,
        help="Number of benchmark iterations to average results over."
    )
    return parser.parse_args()


def main() -> None:
    """
    Main entry point for the benchmark script.
    """
    args = parse_args()
    run_benchmark(args)


if __name__ == "__main__":
    main()
