#!/usr/bin/env python3
"""
MLX-LM Benchmark Tool

This script benchmarks the performance of MLX-LM models by generating synthetic
prompt tokens and capturing performance metrics such as tokens-per-second (TPS),
execution time, and peak memory usage. It supports multiple input values for model,
prompt tokens, and generation tokens. The tool runs benchmarks for each combination,
averages the results over several repetitions, and saves the aggregated results in
the desired output format.
"""

import argparse
import json
import logging
import random
import re
import io
import contextlib
import csv
from typing import Any, Dict, List, Optional, Union

import mlx.core as mx
import mlx.nn as nn
from mlx_lm import load, generate

# Configure logging for better observability
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")


def load_model_tokenizer(model_path: str) -> (Any, Any):
    """
    Load the MLX-LM model and its associated tokenizer.

    Args:
        model_path (str): Path or identifier of the model.

    Returns:
        Tuple[Any, Any]: The loaded model and tokenizer.
    """
    logging.info(f"Loading model from: {model_path}...")
    model, tokenizer = load(model_path)
    return model, tokenizer


def generate_synthetic_tokens(tokenizer: Any, seq_length: int) -> List[int]:
    """
    Generate a synthetic sequence of tokens using the tokenizer's vocabulary.

    Args:
        tokenizer (Any): The tokenizer instance.
        seq_length (int): Desired total number of tokens.

    Returns:
        List[int]: List of token IDs forming the synthetic sequence.
    """
    vocab_size = tokenizer.vocab_size

    # Prepend BOS token if available; otherwise, start with an empty list.
    tokens = [tokenizer.bos_token_id] if tokenizer.bos_token_id is not None else []
    tokens += [random.randint(0, vocab_size - 1) for _ in range(seq_length - len(tokens))]

    return tokens


def parse_metrics(log_output: str) -> Dict[str, Optional[float]]:
    """
    Parse performance metrics from the log output generated by the `generate()` function.

    Args:
        log_output (str): Captured stdout containing performance logs.

    Returns:
        Dict[str, Optional[float]]: Parsed metrics including:
            - prompt_tokens: Number of prompt tokens processed.
            - prompt_tps: Prompt tokens processed per second.
            - response_tokens: Number of tokens generated.
            - response_tps: Generation tokens processed per second.
            - ram_usage: Peak memory usage in GB.
            - exec_time: Estimated execution time (s).
    """
    metrics: Dict[str, Optional[float]] = {
        "prompt_tokens": None,
        "prompt_tps": None,
        "response_tokens": None,
        "response_tps": None,
        "ram_usage": None,
        "exec_time": None
    }

    # Extract prompt tokens and tokens-per-second
    prompt_match = re.search(r"Prompt:\s*(\d+)\s*tokens,\s*([\d.]+)\s*tokens-per-sec", log_output)
    if prompt_match:
        metrics["prompt_tokens"] = int(prompt_match.group(1))
        metrics["prompt_tps"] = float(prompt_match.group(2))

    # Extract generation tokens and tokens-per-second
    generation_match = re.search(r"Generation:\s*(\d+)\s*tokens,\s*([\d.]+)\s*tokens-per-sec", log_output)
    if generation_match:
        metrics["response_tokens"] = int(generation_match.group(1))
        metrics["response_tps"] = float(generation_match.group(2))

    # Extract peak memory usage (GB)
    mem_match = re.search(r"Peak memory:\s*([\d.]+)\s*GB", log_output)
    if mem_match:
        metrics["ram_usage"] = float(mem_match.group(1))

    # Calculate total execution time if metrics are available
    if (metrics["prompt_tokens"] is not None and metrics["prompt_tps"] is not None and
        metrics["response_tokens"] is not None and metrics["response_tps"] is not None):
        metrics["exec_time"] = (metrics["prompt_tokens"] / metrics["prompt_tps"]) + (
            metrics["response_tokens"] / metrics["response_tps"])

    return metrics


def benchmark_performance(model: Any, tokenizer: Any, seq_length: int, max_tokens: int) -> Dict[str, Optional[float]]:
    """
    Run a single benchmark iteration, capturing performance metrics.

    Args:
        model (Any): The loaded MLX-LM model.
        tokenizer (Any): The associated tokenizer.
        seq_length (int): Number of synthetic prompt tokens.
        max_tokens (int): Maximum number of tokens to generate.

    Returns:
        Dict[str, Optional[float]]: Performance metrics from this iteration.
    """
    input_tokens = generate_synthetic_tokens(tokenizer, seq_length)
    output_buffer = io.StringIO()
    with contextlib.redirect_stdout(output_buffer):
        generate(model, tokenizer, input_tokens, max_tokens=max_tokens, verbose=True)
    captured_output = output_buffer.getvalue()
    return parse_metrics(captured_output)


def save_results(results: Union[Dict[str, Any], List[Dict[str, Any]]], output_format: str) -> None:
    """
    Save the benchmark results in the specified output format.

    Args:
        results (Union[Dict[str, Any], List[Dict[str, Any]]]): Benchmark results.
        output_format (str): Format to save the results ("csv", "json", "jsonl", or "md").
    """
    # Ensure results is a list of dictionaries.
    if not isinstance(results, list):
        results = [results]

    if output_format == "json":
        with open("benchmark_results.json", "w") as f:
            json.dump(results, f, indent=4)
        logging.info("Results saved to benchmark_results.json")
    elif output_format == "jsonl":
        with open("benchmark_results.jsonl", "w") as f:
            for res in results:
                f.write(json.dumps(res) + "\n")
        logging.info("Results saved to benchmark_results.jsonl")
    elif output_format == "csv":
        with open("benchmark_results.csv", "w", newline="") as f:
            if results:
                writer = csv.DictWriter(f, fieldnames=results[0].keys())
                writer.writeheader()
                for res in results:
                    writer.writerow(res)
        logging.info("Results saved to benchmark_results.csv")
    elif output_format == "md":
        if results:
            with open("benchmark_results.md", "w") as f:
                headers = list(results[0].keys())
                f.write("| " + " | ".join(headers) + " |\n")
                f.write("|" + "|".join(["---"] * len(headers)) + "|\n")
                for res in results:
                    f.write("| " + " | ".join(str(res[h]) for h in headers) + " |\n")
        logging.info("Results saved to benchmark_results.md")
    else:
        logging.warning(f"Unsupported output format: {output_format}")


class CommaSeparatedIntegers(argparse.Action):
    """
    Custom argparse action to allow comma-separated integers.
    """
    def __call__(self, parser, namespace, values, option_string=None):
        result = []
        for value in values:
            result.extend([int(x) for x in value.split(',') if x])
        setattr(namespace, self.dest, result)


def parse_args() -> argparse.Namespace:
    """
    Parse command-line arguments for configuring the benchmark.

    Returns:
        argparse.Namespace: The parsed command-line arguments.
    """
    parser = argparse.ArgumentParser(description="Benchmark MLX-LM model performance.")

    # Allow multiple models by repeating the -m flag.
    parser.add_argument(
        "-m", "--model", action="append", required=True,
        help="Path to the MLX model to benchmark. Can be specified multiple times for different models."
    )
    parser.add_argument(
        "-p", "--n-prompt", nargs="+", action=CommaSeparatedIntegers, default=[128],
        help="Number of synthetic prompt tokens. Accepts multiple comma-separated values."
    )
    parser.add_argument(
        "-n", "--n-gen", nargs="+", action=CommaSeparatedIntegers, default=[512],
        help="Number of tokens to generate. Accepts multiple comma-separated values."
    )
    parser.add_argument(
        "-o", "--output", type=str, choices=["csv", "json", "jsonl", "md"],
        default="csv", help="Output format for the benchmark results."
    )
    parser.add_argument(
        "-r", "--repetitions", type=int, default=5,
        help="Number of benchmark repetitions to average results over."
    )

    return parser.parse_args()


def run_benchmarks(args: argparse.Namespace) -> List[Dict[str, Any]]:
    """
    Execute the benchmark for each combination of model, prompt tokens, and generation tokens.

    Args:
        args (argparse.Namespace): Command-line arguments.

    Returns:
        List[Dict[str, Any]]: Aggregated benchmark results.
    """
    all_results = []
    for model_path in args.model:
        model, tokenizer = load_model_tokenizer(model_path)
        for n_prompt in args.n_prompt:
            for n_gen in args.n_gen:
                logging.info(f"Benchmarking model: {model_path} | Prompt tokens: {n_prompt} | Generation tokens: {n_gen}")
                # Warmup run
                _ = benchmark_performance(model, tokenizer, n_prompt, n_gen)
                # Benchmark iterations
                metrics_list = []
                for i in range(args.repetitions):
                    logging.info(f"Iteration {i + 1}/{args.repetitions} for model: {model_path}, prompt: {n_prompt}, n_gen: {n_gen}")
                    metrics = benchmark_performance(model, tokenizer, n_prompt, n_gen)
                    metrics_list.append(metrics)
                # Compute average metrics
                avg_metrics = {}
                keys = ["prompt_tokens", "prompt_tps", "response_tokens", "response_tps", "exec_time", "ram_usage"]
                for key in keys:
                    valid_values = [m[key] for m in metrics_list if m[key] is not None]
                    avg_metrics[key] = sum(valid_values) / len(valid_values) if valid_values else None
                result = {
                    "Model": model_path,
                    "n_prompt": n_prompt,
                    "n_gen": n_gen,
                    "Prompt Tokens": int(avg_metrics["prompt_tokens"]),
                    "Prompt TPS": round(avg_metrics["prompt_tps"], 3),
                    "Response Tokens": int(avg_metrics["response_tokens"]),
                    "Response TPS": round(avg_metrics["response_tps"], 3),
                    "Execution Time (s)": round(avg_metrics["exec_time"],3),
                    "Memory Usage (GB)": round(avg_metrics["ram_usage"], 2) if avg_metrics["ram_usage"] is not None else None
                }
                all_results.append(result)
    save_results(all_results, args.output)
    return all_results


def main() -> None:
    """
    Main entry point for the benchmark script.
    """
    args = parse_args()
    run_benchmarks(args)


if __name__ == "__main__":
    main()
